{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from time import sleep\n",
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys\n",
    "\n",
    "# Twitter API Keys\n",
    "consumer_key = 'consumer_key'\n",
    "consumer_secret = 'consumer_secret'\n",
    "access_token = 'access_token'\n",
    "access_token_secret = 'access_token_secret'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API Setup\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe to store tweets\n",
    "new_tweets_df = pd.DataFrame(columns=['UserID', 'TweetID', 'Tweet', 'CreatedAt', 'Spam', 'URLs Expanded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select United Kingdom as query constraint\n",
    "places = api.geo_search(query=\"United Kingdom\", granularity=\"country\")\n",
    "place_id = places[2].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweet_data(tweets):\n",
    "    if not os.path.exists('../Data/collected_tweet_data.txt'):\n",
    "       with open('../Data/collected_tweet_data.txt', 'w', encoding=\"utf-8\") as f:\n",
    "           pass     \n",
    "    with open('../Data/collected_tweet_data.txt', 'r', encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter='\\n')\n",
    "        existing = set()\n",
    "        for line in reader:\n",
    "            line_data = line[0]\n",
    "            existing.add(str(line_data))\n",
    "            \n",
    "    with open('../Data/collected_tweet_data.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        for tweet in tweets:\n",
    "            if tweet.lang == 'en':\n",
    "                tweet_data = str(tweet._json)\n",
    "                if tweet_data not in existing:\n",
    "                    f.write(tweet_data)\n",
    "                    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets():\n",
    "  \n",
    "  # Retrieve tweets\n",
    "  tweets = api.search(q=\"place:%s -filter:retweets\" % place_id)\n",
    "  \n",
    "  save_tweet_data(tweets)\n",
    "  \n",
    "  # Analyze tweet metadata and store\n",
    "  for tweet in tweets:\n",
    "    \n",
    "    # Skip tweet if already logged or not in english\n",
    "    if tweet.id in new_tweets_df.TweetID or tweet.lang != 'en':\n",
    "      break\n",
    "    \n",
    "    # Get any URLs from tweet and expand them\n",
    "    urls = []\n",
    "    if tweet.entities['urls'] != []:\n",
    "      for url in tweet.entities['urls']:\n",
    "        urls.append(url['expanded_url'])\n",
    "        \n",
    "    # Record tweet in dataframe\n",
    "    new_tweets_df.loc[len(new_tweets_df.index)] = [tweet.user.id, tweet.id, tweet.text, tweet.created_at, 'Unknown', urls]\n",
    "    tweets = None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rate_limit(api, wait=True, buffer=.1):\n",
    "    \"\"\"\n",
    "    Tests whether the rate limit of the last request has been reached.\n",
    "    :param api: The `tweepy` api instance.\n",
    "    :param wait: A flag indicating whether to wait for the rate limit reset\n",
    "                 if the rate limit has been reached.\n",
    "    :param buffer: A buffer time in seconds that is added on to the waiting\n",
    "                   time as an extra safety margin.\n",
    "    :return: True if it is ok to proceed with the next request. False otherwise.\n",
    "    \"\"\"\n",
    "    # Get number of remaining requests\n",
    "    remaining = int(api.last_response.headers.get('x-rate-limit-remaining'))\n",
    "    # Check if limit has been reached\n",
    "    if remaining == 0:\n",
    "        limit = int(api.last_response.headers.get('x-rate-limit-limit'))\n",
    "        reset = int(api.last_response.headers.get('x-rate-limit-reset'))\n",
    "        # Get UTC time\n",
    "        reset = datetime.datetime.fromtimestamp(reset)\n",
    "        print(\"Paused, will resume at\", reset)\n",
    "\n",
    "        if wait:\n",
    "            # Determine delay and sleep\n",
    "            delay = (reset - datetime.datetime.now()).total_seconds() + buffer\n",
    "            print(\"Sleeping for {}s...\".format(delay), 'at:', datetime.datetime.now())\n",
    "            sleep(delay)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets(df):\n",
    "    df = df.drop_duplicates(subset=['TweetID'])\n",
    "    \n",
    "    if not os.path.exists('../Data/collected_tweets.csv'):\n",
    "        df.to_csv('../Data/collected_tweets.csv', mode='w', index=False)\n",
    "    else:\n",
    "        df.to_csv('../Data/collected_tweets.csv', mode='a', index=False, header=False)\n",
    "        \n",
    "    \n",
    "    print('Tweets updated at:', datetime.datetime.now())\n",
    "    \n",
    "    return df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates():\n",
    "    try:\n",
    "        collected_tweets_df = pd.read_csv('../Data/collected_tweets.csv')\n",
    "        print('Unfiltered tweets:', len(collected_tweets_df))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    collected_tweets_df = collected_tweets_df.drop_duplicates(subset=['TweetID'])\n",
    "    print('Unique tweets:', len(collected_tweets_df))\n",
    "    collected_tweets_df.to_csv('../Data/collected_tweets.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to start collecting tweets automatically within API Rate limits\n",
    "start_time = datetime.datetime.now()\n",
    "loop_time = start_time\n",
    "try:\n",
    "  \n",
    "  while test_rate_limit(api):\n",
    "    get_tweets()\n",
    "    test_time = datetime.datetime.now()\n",
    "    if (test_time - loop_time).seconds > 300:\n",
    "      new_tweets_df = save_tweets(new_tweets_df)\n",
    "      loop_time = datetime.datetime.now()\n",
    "  \n",
    "  if not test_rate_limit(api):\n",
    "    print('Limit Reached at', datetime.datetime.now())\n",
    "  \n",
    "except Exception as e:\n",
    "  new_tweets_df = save_tweets(new_tweets_df)\n",
    "  print(\"Stopped unexpectedly\")\n",
    "  print(e)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "141244049137c618073c79afd0f9a58880087fb35bece71bcaf6735ec1c6597d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
